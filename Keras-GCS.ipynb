{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instalación desde cero de librerías de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall tensorflow \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall sklearn\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall keras\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall h5py \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall tensorflow-transform \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade tensorflow\n",
    "pip install --upgrade sklearn\n",
    "\n",
    "# Si no funciona sklearn, hacer uninstall e install: pip uninstall scikit-learn\n",
    "#pip uninstall scikit-learn\n",
    "#pip install scikit-learn\n",
    "\n",
    "# Comprobar paquetes instalados con:\n",
    "# pip freeze\n",
    "\n",
    "pip install keras\n",
    "pip install h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejemplo de importar csv desde GCS\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from StringIO import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "\n",
    "# Read csv file from GCS into a variable\n",
    "%storage read --object gs://analiticauniversal/DatasetsTF/creditcards.csv --variable creditcards\n",
    "\n",
    "# Store in a pandas dataframe\n",
    "df = pd.read_csv(StringIO(creditcards))\n",
    "dataset = df.as_matrix()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[:,:-1], dataset[:,-1], test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import tensorflow.contrib.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1,l2\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.constraints import max_norm\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "from os.path import abspath\n",
    "import os\n",
    "\n",
    "\n",
    "# Disable info warnings from TF\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "NOW = datetime.now().strftime(\"%Y-%m-%d--%Hh%Mm%Ss\")\n",
    "ROOT_LOGDIR = 'gs://analiticauniversal/LogsTF'\n",
    "LOG_DIR = '{}/run-{}'.format(ROOT_LOGDIR, NOW)\n",
    "OUTPUT_FILE = LOG_DIR + '/results.txt'\n",
    "\n",
    "LOCAL_DIR = '/run-{}'.format(NOW)\n",
    "CSV_LOG = LOCAL_DIR + '/training.log'\n",
    "CKPT = LOCAL_DIR + '/ckpt.hdf5'\n",
    "MODEL = LOCAL_DIR + '/model.h5'\n",
    "\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)\n",
    "\n",
    "if tf.gfile.Exists(LOCAL_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOCAL_DIR)\n",
    "tf.gfile.MakeDirs(LOCAL_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 500\n",
    "epochs = 10\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Parameters for early stopping (increase them when using auc scores)\n",
    "DELTA = 1e-6\n",
    "PATIENCE = 200\n",
    "\n",
    "# Auc callback interval\n",
    "AUCS_INTERVAL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(CSV_LOG)\n",
    "early_stopping = EarlyStopping(min_delta = DELTA, patience = PATIENCE )\n",
    "#ckpt = ModelCheckpoint(filepath = CKPT, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, kernel_initializer=\"he_normal\", input_shape=(30,))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 5)                 155       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 187\n",
      "Trainable params: 177\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n",
      "Train on 230692 samples, validate on 25633 samples\n",
      "Epoch 1/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.5853 - acc: 0.7456 - val_loss: 0.2113 - val_acc: 0.9981\n",
      "Epoch 2/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.1419 - acc: 0.9857 - val_loss: 0.0450 - val_acc: 0.9981\n",
      "Epoch 3/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0531 - acc: 0.9980 - val_loss: 0.0177 - val_acc: 0.9981\n",
      "Epoch 4/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0354 - acc: 0.9982 - val_loss: 0.0144 - val_acc: 0.9981\n",
      "Epoch 5/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0293 - acc: 0.9982 - val_loss: 0.0137 - val_acc: 0.9981\n",
      "Epoch 6/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0262 - acc: 0.9982 - val_loss: 0.0138 - val_acc: 0.9981\n",
      "Epoch 7/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0241 - acc: 0.9982 - val_loss: 0.0138 - val_acc: 0.9981\n",
      "Epoch 8/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0221 - acc: 0.9982 - val_loss: 0.0141 - val_acc: 0.9981\n",
      "Epoch 9/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0209 - acc: 0.9982 - val_loss: 0.0141 - val_acc: 0.9981\n",
      "Epoch 10/10\n",
      "230692/230692 [==============================] - 1s - loss: 0.0191 - acc: 0.9982 - val_loss: 0.0147 - val_acc: 0.9981\n",
      "Test loss: 0.010315375824\n",
      "Test accuracy: 0.998700888312 \n",
      "\n",
      "Test AUC: 58.6012059732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = X_train\n",
    "y_train = y_train\n",
    "x_val = X_val\n",
    "y_val = y_val\n",
    "x_test = X_test\n",
    "y_test = y_test\n",
    "\n",
    "input_dim = dataset.shape[1] - 1\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5,input_shape=(input_dim,), init='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[csv_logger, early_stopping])\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1], \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict_proba(x_test, verbose = 0)\n",
    "y_score = y_pred[:,1]\n",
    "auc = roc_auc_score(y_true=y_test, y_score=y_score)\n",
    "auc *=100\n",
    "print(\"Test AUC:\", auc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout = tf.gfile.Open(name=OUTPUT_FILE, mode='w')  \n",
    "json_string = model.to_json() \n",
    "print(\"Network structure (json format)\", \"\\n\")\n",
    "print(json_string, \"\\n\")\n",
    "print(\"Hyperparameters\", \"\\n\")\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Dropout rate:\", dropout_rate, \"\\n\")\n",
    "model.summary()\n",
    "\"\"\"\n",
    "print(\"Validation AUCs during training\", \"\\n\")\n",
    "for i in range(len(ival.aucs)):\n",
    "    print(\"\\t\",\"Epoch\", str(i), \"- val_auc:\", ival.aucs[i], \" - loss:\", ival.losses[i])\n",
    "\"\"\"\n",
    "print('\\n','Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]*100, '\\n')\n",
    "print('Test AUC:', auc)\n",
    "sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.gfile.Copy(oldpath=CSV_LOG , newpath=LOG_DIR + '/training.log' )\n",
    "#tf.gfile.Copy(oldpath=CKPT , newpath=LOG_DIR + '/ckpt.hdf5' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf LOCAL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
